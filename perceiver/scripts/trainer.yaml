trainer:
  callbacks:
  - class_path: pytorch_lightning.callbacks.lr_monitor.LearningRateMonitor
    init_args:
      logging_interval: step
      log_momentum: false
  - class_path: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint
    init_args:
      filename: '{epoch:03d}-{val_loss:.3f}-{val_dice:.9f}'
      monitor: 'val_loss'
      mode: min
      # save_weights_only: true
      save_last: true
      save_top_k: 50
      every_n_epochs: 1
      save_on_train_epoch_end: false
  default_root_dir: logs
  strategy: ddp_find_unused_parameters_false
